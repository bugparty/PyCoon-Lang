#!/usr/bin/bash
declare -a failed_tests
declare -a passing_tests
actual_output_dir="./actual_outputs"
# Path to the executable
executable_path="/usr/local/bin/mil_run"
mkdir -p "$actual_output_dir"

run_e2e_test() {
    filename=$(basename "$1")
    dirname=$(dirname "$1")
  # Print the file name
  echo -e "\e[34mFile name: $filename \e[0m "

  ./onion < "$1"
  error=$?
  if [ "$error" -ne 0 ]; then
    echo -e "\e[31mError found in: $filename ,stop testing. \e[0m"
    failed_tests+=("$filename")
  else
      # Generate paths for the expected output and actual output files
    expected_output_file="$dirname/$filename.output"
    input_file="$dirname/$filename.input"
    actual_output_file="$actual_output_dir/output_$filename.txt"
    # Execute the program with the input file and save the output
    "$executable_path" "a.mil" < "$input_file" > "$actual_output_file"
    # Compare the actual output with the expected output
    if diff -B -b "$expected_output_file" "$actual_output_file" > /dev/null; then
       echo -e "\e[32mTest succees in : $filename \e[0m"
      passing_tests+=("$filename")
    else
      echo -e "\e[31mTest case $filename: FAIL \e[0m"
      failed_tests+=("$filename")
      diff "-u" "--color" "$expected_output_file" "$actual_output_file"
    fi
   
   
  fi
}
run_test() {
    filename=$(basename "$1")
  
  # Print the file name
  echo -e "\e[34mFile name: $filename \e[0m "

  cat $1 | ./onion
  error=$?
  if [ "$error" -ne 0 ]; then
    echo -e "\e[31mError found,stop testing. \e[0m"
    failed_tests+=("$filename")
  else
    echo -e "\e[32mTest succees in : $filename \e[0m"
    passing_tests+=("$filename")
  fi
}
expect_fail() {
    filename=$(basename "$1")
  
  # Print the file name
  echo -e "\e[34mFile name: $filename \e[0m "

  cat $1 | ./onion
  error=$?
  if [ "$error" -ne 0 ]; then
    echo -e "\e[32mTest succees in : $filename \e[0m"
    passing_tests+=("$filename")
  else
    echo -e "\e[31mError found,stop testing. \e[0m"
    failed_tests+=("$filename")
  fi
}
make
for PASSINGFILE in ../doc/language_samples/IR_test_case/*; do run_test $PASSINGFILE; done;
for PASSINGFILE in tests/IR_auto_tests/*.pyco; do run_e2e_test $PASSINGFILE; done;
for FAILINGFILE in ../doc/language_samples/test_cases/parser_fail/*; do expect_fail $FAILINGFILE; done;

echo ""
echo -e "\e[33mSummary:\e[0m"
echo "Total tests run: $(( ${#failed_tests[@]} + ${#passing_tests[@]} ))"
echo "Tests passed: $(( ${#passing_tests[@]} ))"
echo "Tests failed: $(( ${#failed_tests[@]} ))"

if [ ${#failed_tests[@]} -gt 0 ]; then
    echo -e "\e[31mFailed tests:\e[0m"
    # for failed_test in "${failed_tests[@]}"; do
    #     echo "$failed_test"
    # done
    echo "${failed_tests[*]}" 
    exit -2
fi
#rm -rf "$actual_output_dir"